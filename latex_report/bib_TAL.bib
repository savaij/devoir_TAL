@misc{ilpost2022,
  title = {{{ARTeLab}}/ilpost · {{Datasets}} at {{Hugging Face}}},
  year = {2022},
  month = oct,
  url = {https://huggingface.co/datasets/ARTeLab/ilpost},
  urldate = {2024-04-21},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/sava/Zotero/storage/UVVJ5B6X/ilpost.html}
}

@misc{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/1908.10084v1},
  urldate = {2024-04-20},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  langid = {english},
  file = {/Users/sava/Zotero/storage/CH5TYAS5/Reimers e Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf}
}

@misc{sentenceBERT2024,
  title = {nickprock/sentence-bert-base-italian-xxl-uncased · {{Hugging Face}}},
  year = {2024},
  month = jan,
  url = {https://huggingface.co/nickprock/sentence-bert-base-italian-xxl-uncased},
  urldate = {2024-04-20},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/sava/Zotero/storage/984D3AMY/sentence-bert-base-italian-xxl-uncased.html}
}
