% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{reimers2019}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=da9947b27b0d1c4c912e2b391426d443}{%
           family={Reimers},
           familyi={R\bibinitperiod},
           given={Nils},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ea70b44be1920f6e8c379706bc7d047a}{%
           family={Gurevych},
           familyi={G\bibinitperiod},
           given={Iryna},
           giveni={I\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2466af6376c9c5e3658dcc6692351f3e}
      \strng{fullhash}{2466af6376c9c5e3658dcc6692351f3e}
      \strng{bibnamehash}{2466af6376c9c5e3658dcc6692351f3e}
      \strng{authorbibnamehash}{2466af6376c9c5e3658dcc6692351f3e}
      \strng{authornamehash}{2466af6376c9c5e3658dcc6692351f3e}
      \strng{authorfullhash}{2466af6376c9c5e3658dcc6692351f3e}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.}
      \field{journaltitle}{arXiv.org}
      \field{langid}{english}
      \field{month}{8}
      \field{shorttitle}{Sentence-{{BERT}}}
      \field{title}{Sentence-{{BERT}}: {{Sentence Embeddings}} using {{Siamese BERT-Networks}}}
      \field{urlday}{20}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/sava/Zotero/storage/CH5TYAS5/Reimers e Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/1908.10084v1
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/1908.10084v1
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

